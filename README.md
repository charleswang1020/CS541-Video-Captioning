# CS541-2019S-Video Captioning
WPI CS 541 Deep Learning Project 

## Description
The accurate automated captioning videos is an important task for both making video collection more accessible to human users and for making video content more accessible to the visually impaired. This task can be segmented into two subtasks: audio encoding into language and visual encoding into language. In this study we focus on the task of expressing the visual content of the videos in the English language. One of the more common frameworks for attempting this task is an encoder-decoder framework. We use this framework to implement and train from scratch a video captioning RNN (GAN) to investigate a current state of the art technique. To improve the performance of the technique we experiment with augmenting this approach with some of the newest pre-trained CNN models for our initial image feature extraction and use modern regularization methods. To demonstrate the effectiveness of our approach we test our method on the MSR-VTT dataset.

## Method
CNN-RNN (GAN) is the based approach with the using of the encoder-decoder framework. This category of approaches leverage advanced pre-trained CNNs to efficiently and accurately preprocess the image component of the video data and trains an RNN to collect data from multiple frames and then generate the sentences. This approach is augmented with some of the newest pre-trained CNN models for our initial image feature extraction.

![](/images/CS541%20Project%20Proposal.jpg)

## Data Source



## Video Demo


## Presentation Slides
