# CS541-2019S-Video Captioning
WPI CS 541 Deep Learning Project 

## Description
Captioning videos is to make video collection more accessible to human users and for making video content more accessible to the visually impaired. This task is to do the visual encoding into language. It focus on the task of expressing the visual content of the videos in the English language. One of the more common frameworks for attempting this task is an encoder-decoder framework. This framework is used to implement and train from scratch a video captioning RNN (GAN) to investigate a current state of the art technique. To improve the performance of the technique we experiment with augmenting this approach with some of the newest pre-trained CNN models for our initial image feature extraction and use modern regularization methods. To demonstrate the effectiveness of our approach we test our method on the MSR-VTT dataset.

## Requirements
OpenCV (Python)
* pip install python-opencv

## Method
CNN-RNN (GAN) is the based approach with the using of the encoder-decoder framework. This category of approaches leverage advanced pre-trained CNNs to efficiently and accurately preprocess the image component of the video data and trains an RNN to collect data from multiple frames and then generate the sentences. This approach is augmented with some of the newest pre-trained CNN models for our initial image feature extraction.

![](/images/CS541%20Project%20Proposal.jpg)
## Data Source


## Video Demo


## Presentation Slides
